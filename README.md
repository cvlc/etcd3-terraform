# `etcd3-terraform`

A terraform recipe, forked from Monzo's [etcd3-terraform](https://github.com/monzo/etcd3-terraform) and updated in order to provide easy deployment of a non-Kubernetes-resident etcd cluster on AWS for Ondat.

## Stack ðŸŽ®

This will create a new VPC and a set of 3 Auto Scaling Groups each running Debian stable by default. These ASGs are distributed over 3 Availability Zones detected from the current region in use (eg. passed via `AWS_REGION` environment variable). All resources are deployed into a VPC that can either be created by setting the `vpc_id` variable to `create` or chosen by setting `vpc_id` to the ID of an existing VPC. 

This will also create a local Route 53 zone for the domain you pick and bind it to the VPC so its records can be resolved. This domain does not need to be registered. An `SRV` record suitable for etcd discovery is also created as well as a Lambda function which monitors ASG events and creates `A` records for each member of the cluster.

A Network Load Balancer will be created for clients of the etcd cluster. It wraps all of the auto-scaling group instances on port `2379` with a health check to ensure that only functional instances are presented.

### High Availability
As mentioned above, the default size of the cluster is 3 nodes - in a highly available environment, this means that only 2 node failures will trigger a catastrophic cluster failure. In order to prevent this, it's suggested to use a larger cluster in any real-world scenario - 5, 7 or 9 nodes should be sufficient depending on risk appetite.

### Elasticity
Scaling out is as easy as increasing the size of the cluster via the aforementioned variable. When scaling down/in, it is suggested to destroy the extreneous instances and autoscaling groups manually via `terraform destroy -target=...` after removing the member from the cluster using `etcdctl` before running another `terraform apply`. Future work could implement lifecycle hooks and autoscaling to make this more automated.

### Backups
Volume snapshots are taken automatically of each node, every day at 2am. A week of snapshots is retained for each node. In order to restore from snapshot, take down the cluster and manually replace each EBS volume. Use `terraform import` to import the new volumes into the state to reconcile from the Terraform end. 

## Security ðŸ”’
In this distribution, we've:
- encrypted all etcd and root volumes
- encryped and authenticated all etcd traffic between peers and clients
- locked down network access to the minimum
- used a modern, stable default AMI

This makes for a secure base configuration. That said, there are some improvements that could be made:

enable offline mode for air-gapped environments by:
- remove the dependency on S3 and the outbound security group rule for port 443 that is necessary for it
- remove the dependency on GitHub for etcd releases for the same

other possible improvements are:
- remove the rule allowing for SSH access from any node and replace with a known good client range
- reduce AWS permissions to a minimum in included policies

It is suggested that this is deployed to private subnets only within a VPC (`subnet_type` to `Private` after tagging private subnets as `Private=true`) and that the `associate_public_ips` variable is kept to false. With `vpc_id=create` this will create a new, appropriately-tagged private VPC. This is the default behaviour.

### Authentication
The etcd nodes authenticate with each other via individual TLS certificates and keys. Clients authenticate using a single certificate. Role Based Access Control [is possible with further configuration via etcd itself](https://etcd.io/docs/v3.5/op-guide/authentication/rbac/).

### Certificates
A CA and several certificates for peers, servers and clients are generated by Terraform and stored in the state file. It is therefore suggested that the state file is stored securely (and ideally remotely, eg. in an encrypted S3 bucket with limited access). Certificates are valid for 5 years (for the CA) and 1 year (for others). At the moment, the renewal process requires replacing the nodes one-at-a-time after the certificates have been destroyed and re-created in terraform - this should be done carefully using `terraform destroy -target=...` and `terraform apply -target=...` for each of the resources in series, spacing out the node replacements to ensure that quorum is not broken. Replacing the CA certificate will require manually copying the new certificates to each instance and restarting the `etcd-member` systemd job to ensure that the cluster remains in-sync through the terraform node replacement process.

The client certificate must be used to authenticate with the server when communicating with etcd from allowed clients (within the cidr range in `client_cidrs`). The certificate and key will be generated by Terraform and placed in the current working directory, named client.pem and client.key respectively. 

## How to configure and deploy ðŸ•¹

The file `variables.tf` declares the Terraform variables required to run this stack. Almost everything has a default - the region will be detected from the `AWS_REGION` environment variable and it will span across the maximum available zones within your preferred region. You will be asked to provide an SSH public key to launch the stack. Variables can all be overridden in a `terraform.tfvars` file or by passing runtime parameters.

Note that if you are creating a VPC with `vpc_id=create` you may need to initialize it first, before the rest of this module. To do so, simply:
```
terraform apply -target=module.vpc
terraform apply
```

### Maintenance
etcd is configured with a 100GB data disk per node on Amazon EBS SSDs, a `revision` auto compaction mode and a retention of `20000`. An automatic cronjob runs on each node to ensure defragmentation happens at least once every month, this briefly blocks reads/writes on a single node at a time from 3:05am on a different day of the month for each node. It's configured with a backend space quota of `8589934592` bytes. This should translate to a fairly 'hands-off' maintenance schedule.

For further details of what these values and settings mean, refer to [etcd's official documentation](https://etcd.io/docs/v3.5/op-guide/maintenance/). 

## How to run etcdctl ðŸ”§
We presume that whatever system you choose to run these commands on can connect to the NLB (ie. if you're using a private subnet, your client machine is within the VPC or connected via a VPN).

First, install the CA certificate to your client machine. On Ubuntu/Debian, this can be done by copying `ca.pem` to `/usr/local/share/ca-certificates/my-etcd-ca.crt` and running `update-ca-certificates`.  

You're now ready to test etcdctl functionality - replace `$insert_nlb_address` with the URL of the NLB. 

```
$ ETCDCTL_API=3 ETCDCTL_CERT=client.pem ETCDCTL_KEY=client.key ETCDCTL_ENDPOINTS="https://$insert_nlb_address:2379" etcdctl member list
25f97d08c726ed1, started, peer-2, https://peer-2.ondat.eu-west-2.i.development.mycompany.local:2380, https://peer-2.ondat.eu-west-2.i.development.mycompany.local:2379, false
326a6d27c048c8ea, started, peer-1, https://peer-1.ondat.eu-west-2.i.development.mycompany.local:2380, https://peer-1.ondat.eu-west-2.i.development.mycompany.local:2379, false
38308ae09ffc8b32, started, peer-0, https://peer-0.ondat.eu-west-2.i.development.mycompany.local:2380, https://peer-0.ondat.eu-west-2.i.development.mycompany.local:2379, false
```
## How to [benchmark](https://etcd.io/docs/v3.5/op-guide/performance/) etcd in your environment ðŸ“Š

### Prep
Be sure that you have `go` installed and `$GOPATH` correctly set with `$GOPATH/bin` in your `$PATH` in addition to being able to run `etcdctl` successfully as above.
```
$ go get go.etcd.io/etcd/v3/tools/benchmark
```

Note that performance will vary significantly depending on the client machine you run the benchmarks from - running them over the internet, even through a VPN, does not provide equitable performance to running directly from inside your VPC. 

### Benchmark the write rate to leader
```
$ benchmark --endpoints="https://$insert_nlb_address:2379" --cert client.pem --key client.key --target-leader --conns=1 --clients=1 put --key-size=8 --sequential-keys --total=10000 --val-size=256

$ benchmark --endpoints="https://$insert_nlb_address:2379" --cert client.pem --key client.key --target-leader --conns=100 --clients=1000 put --key-size=8 --sequential-keys --total=100000 --val-size=256
```

### Benchmark writes to all members
```
$ benchmark --endpoints="https://$insert_nlb_address:2379" --cert client.pem --key client.key --conns=100 --clients=1000 put --key-size=8 --sequential-keys --total=100000 --val-size=256
```

### Benchmark single connection reads
```
$ benchmark --endpoints="https://$insert_nlb_address:2379" --cert client.pem --key client.key --conns=1 --clients=1 range YOUR_KEY --consistency=l --total=10000

$ benchmark --endpoints="https://$insert_nlb_address:2379" --cert client.pem --key client.key --conns=1 --clients=1 range YOUR_KEY --consistency=s --total=10000
```

### Benchmark many concurrent reads
```
$ benchmark --endpoints="https://$insert_nlb_address:2379" --cert client.pem --key client.key --conns=100 --clients=1000 range YOUR_KEY --consistency=l --total=100000

$ benchmark --endpoints="https://$insert_nlb_address:2379" --cert client.pem --key client.key --conns=100 --clients=1000 range YOUR_KEY --consistency=s --total=100000
```
